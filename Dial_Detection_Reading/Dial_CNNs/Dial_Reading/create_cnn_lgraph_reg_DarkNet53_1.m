% Using DarkNet53 as the base architecture but doing regression

function lgraph = create_cnn_lgraph_reg_DarkNet53_1(num_outputs)
    % Default Value
    if ~exist('output_size', 'var')
        num_outputs = 1;
    end

%     Create Deep Learning Network Architecture with Pretrained Parameters
%     Script for creating the layers for a deep learning network with the following properties:
%     Number of layers: 184
%     Number of connections: 206
%     Pretrained parameters file: C:\BU\Semeter_8_(Spring2023)\Blender\params_2023_04_24__09_28_38.mat
%     Run the script to create the layers in the workspace variable lgraph.
%     To learn more, see Generate MATLAB Code From Deep Network Designer.
%     Auto-generated by MATLAB on 24-Apr-2023 09:28:46
%     Load the Pretrained Parameters
    params = load("C:\BU\Semeter_8_(Spring2023)\Blender\params_2023_04_24__09_28_38.mat");
    
    % Create Layer Graph
    % Create the layer graph variable to contain the network layers.
    lgraph = layerGraph();
    
    % Add Layer Branches
    % Add the branches of the network to the layer graph. Each branch is a linear array of layers.
    tempLayers = [
        imageInputLayer([256 256 3],"Name","input","Normalization","rescale-zero-one","Max",params.input.Max,"Min",params.input.Min)
        convolution2dLayer([3 3],32,"Name","conv1","Padding","same","Bias",params.conv1.Bias,"Weights",params.conv1.Weights)
        batchNormalizationLayer("Name","batchnorm1","Offset",params.batchnorm1.Offset,"Scale",params.batchnorm1.Scale,"TrainedMean",params.batchnorm1.TrainedMean,"TrainedVariance",params.batchnorm1.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu1")
        convolution2dLayer([3 3],64,"Name","conv2","Padding",[1 0 1 0],"Stride",[2 2],"Bias",params.conv2.Bias,"Weights",params.conv2.Weights)
        batchNormalizationLayer("Name","batchnorm2","Offset",params.batchnorm2.Offset,"Scale",params.batchnorm2.Scale,"TrainedMean",params.batchnorm2.TrainedMean,"TrainedVariance",params.batchnorm2.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu2")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],32,"Name","conv3","Padding","same","Bias",params.conv3.Bias,"Weights",params.conv3.Weights)
        batchNormalizationLayer("Name","batchnorm3","Offset",params.batchnorm3.Offset,"Scale",params.batchnorm3.Scale,"TrainedMean",params.batchnorm3.TrainedMean,"TrainedVariance",params.batchnorm3.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu3")
        convolution2dLayer([3 3],64,"Name","conv4","Padding","same","Bias",params.conv4.Bias,"Weights",params.conv4.Weights)
        batchNormalizationLayer("Name","batchnorm4","Offset",params.batchnorm4.Offset,"Scale",params.batchnorm4.Scale,"TrainedMean",params.batchnorm4.TrainedMean,"TrainedVariance",params.batchnorm4.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu4")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        additionLayer(2,"Name","res1")
        convolution2dLayer([3 3],128,"Name","conv5","Padding",[1 0 1 0],"Stride",[2 2],"Bias",params.conv5.Bias,"Weights",params.conv5.Weights)
        batchNormalizationLayer("Name","batchnorm5","Offset",params.batchnorm5.Offset,"Scale",params.batchnorm5.Scale,"TrainedMean",params.batchnorm5.TrainedMean,"TrainedVariance",params.batchnorm5.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu5")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],64,"Name","conv6","Padding","same","Bias",params.conv6.Bias,"Weights",params.conv6.Weights)
        batchNormalizationLayer("Name","batchnorm6","Offset",params.batchnorm6.Offset,"Scale",params.batchnorm6.Scale,"TrainedMean",params.batchnorm6.TrainedMean,"TrainedVariance",params.batchnorm6.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu6")
        convolution2dLayer([3 3],128,"Name","conv7","Padding","same","Bias",params.conv7.Bias,"Weights",params.conv7.Weights)
        batchNormalizationLayer("Name","batchnorm7","Offset",params.batchnorm7.Offset,"Scale",params.batchnorm7.Scale,"TrainedMean",params.batchnorm7.TrainedMean,"TrainedVariance",params.batchnorm7.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu7")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res2");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],64,"Name","conv8","Padding","same","Bias",params.conv8.Bias,"Weights",params.conv8.Weights)
        batchNormalizationLayer("Name","batchnorm8","Offset",params.batchnorm8.Offset,"Scale",params.batchnorm8.Scale,"TrainedMean",params.batchnorm8.TrainedMean,"TrainedVariance",params.batchnorm8.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu8")
        convolution2dLayer([3 3],128,"Name","conv9","Padding","same","Bias",params.conv9.Bias,"Weights",params.conv9.Weights)
        batchNormalizationLayer("Name","batchnorm9","Offset",params.batchnorm9.Offset,"Scale",params.batchnorm9.Scale,"TrainedMean",params.batchnorm9.TrainedMean,"TrainedVariance",params.batchnorm9.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu9")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        additionLayer(2,"Name","res3")
        convolution2dLayer([3 3],256,"Name","conv10","Padding",[1 0 1 0],"Stride",[2 2],"Bias",params.conv10.Bias,"Weights",params.conv10.Weights)
        batchNormalizationLayer("Name","batchnorm10","Offset",params.batchnorm10.Offset,"Scale",params.batchnorm10.Scale,"TrainedMean",params.batchnorm10.TrainedMean,"TrainedVariance",params.batchnorm10.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu10")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],128,"Name","conv11","Padding","same","Bias",params.conv11.Bias,"Weights",params.conv11.Weights)
        batchNormalizationLayer("Name","batchnorm11","Offset",params.batchnorm11.Offset,"Scale",params.batchnorm11.Scale,"TrainedMean",params.batchnorm11.TrainedMean,"TrainedVariance",params.batchnorm11.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu11")
        convolution2dLayer([3 3],256,"Name","conv12","Padding","same","Bias",params.conv12.Bias,"Weights",params.conv12.Weights)
        batchNormalizationLayer("Name","batchnorm12","Offset",params.batchnorm12.Offset,"Scale",params.batchnorm12.Scale,"TrainedMean",params.batchnorm12.TrainedMean,"TrainedVariance",params.batchnorm12.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu12")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res4");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],128,"Name","conv13","Padding","same","Bias",params.conv13.Bias,"Weights",params.conv13.Weights)
        batchNormalizationLayer("Name","batchnorm13","Offset",params.batchnorm13.Offset,"Scale",params.batchnorm13.Scale,"TrainedMean",params.batchnorm13.TrainedMean,"TrainedVariance",params.batchnorm13.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu13")
        convolution2dLayer([3 3],256,"Name","conv14","Padding","same","Bias",params.conv14.Bias,"Weights",params.conv14.Weights)
        batchNormalizationLayer("Name","batchnorm14","Offset",params.batchnorm14.Offset,"Scale",params.batchnorm14.Scale,"TrainedMean",params.batchnorm14.TrainedMean,"TrainedVariance",params.batchnorm14.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu14")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res5");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],128,"Name","conv15","Padding","same","Bias",params.conv15.Bias,"Weights",params.conv15.Weights)
        batchNormalizationLayer("Name","batchnorm15","Offset",params.batchnorm15.Offset,"Scale",params.batchnorm15.Scale,"TrainedMean",params.batchnorm15.TrainedMean,"TrainedVariance",params.batchnorm15.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu15")
        convolution2dLayer([3 3],256,"Name","conv16","Padding","same","Bias",params.conv16.Bias,"Weights",params.conv16.Weights)
        batchNormalizationLayer("Name","batchnorm16","Offset",params.batchnorm16.Offset,"Scale",params.batchnorm16.Scale,"TrainedMean",params.batchnorm16.TrainedMean,"TrainedVariance",params.batchnorm16.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu16")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res6");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],128,"Name","conv17","Padding","same","Bias",params.conv17.Bias,"Weights",params.conv17.Weights)
        batchNormalizationLayer("Name","batchnorm17","Offset",params.batchnorm17.Offset,"Scale",params.batchnorm17.Scale,"TrainedMean",params.batchnorm17.TrainedMean,"TrainedVariance",params.batchnorm17.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu17")
        convolution2dLayer([3 3],256,"Name","conv18","Padding","same","Bias",params.conv18.Bias,"Weights",params.conv18.Weights)
        batchNormalizationLayer("Name","batchnorm18","Offset",params.batchnorm18.Offset,"Scale",params.batchnorm18.Scale,"TrainedMean",params.batchnorm18.TrainedMean,"TrainedVariance",params.batchnorm18.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu18")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res7");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],128,"Name","conv19","Padding","same","Bias",params.conv19.Bias,"Weights",params.conv19.Weights)
        batchNormalizationLayer("Name","batchnorm19","Offset",params.batchnorm19.Offset,"Scale",params.batchnorm19.Scale,"TrainedMean",params.batchnorm19.TrainedMean,"TrainedVariance",params.batchnorm19.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu19")
        convolution2dLayer([3 3],256,"Name","conv20","Padding","same","Bias",params.conv20.Bias,"Weights",params.conv20.Weights)
        batchNormalizationLayer("Name","batchnorm20","Offset",params.batchnorm20.Offset,"Scale",params.batchnorm20.Scale,"TrainedMean",params.batchnorm20.TrainedMean,"TrainedVariance",params.batchnorm20.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu20")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res8");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],128,"Name","conv21","Padding","same","Bias",params.conv21.Bias,"Weights",params.conv21.Weights)
        batchNormalizationLayer("Name","batchnorm21","Offset",params.batchnorm21.Offset,"Scale",params.batchnorm21.Scale,"TrainedMean",params.batchnorm21.TrainedMean,"TrainedVariance",params.batchnorm21.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu21")
        convolution2dLayer([3 3],256,"Name","conv22","Padding","same","Bias",params.conv22.Bias,"Weights",params.conv22.Weights)
        batchNormalizationLayer("Name","batchnorm22","Offset",params.batchnorm22.Offset,"Scale",params.batchnorm22.Scale,"TrainedMean",params.batchnorm22.TrainedMean,"TrainedVariance",params.batchnorm22.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu22")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res9");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],128,"Name","conv23","Padding","same","Bias",params.conv23.Bias,"Weights",params.conv23.Weights)
        batchNormalizationLayer("Name","batchnorm23","Offset",params.batchnorm23.Offset,"Scale",params.batchnorm23.Scale,"TrainedMean",params.batchnorm23.TrainedMean,"TrainedVariance",params.batchnorm23.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu23")
        convolution2dLayer([3 3],256,"Name","conv24","Padding","same","Bias",params.conv24.Bias,"Weights",params.conv24.Weights)
        batchNormalizationLayer("Name","batchnorm24","Offset",params.batchnorm24.Offset,"Scale",params.batchnorm24.Scale,"TrainedMean",params.batchnorm24.TrainedMean,"TrainedVariance",params.batchnorm24.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu24")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res10");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],128,"Name","conv25","Padding","same","Bias",params.conv25.Bias,"Weights",params.conv25.Weights)
        batchNormalizationLayer("Name","batchnorm25","Offset",params.batchnorm25.Offset,"Scale",params.batchnorm25.Scale,"TrainedMean",params.batchnorm25.TrainedMean,"TrainedVariance",params.batchnorm25.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu25")
        convolution2dLayer([3 3],256,"Name","conv26","Padding","same","Bias",params.conv26.Bias,"Weights",params.conv26.Weights)
        batchNormalizationLayer("Name","batchnorm26","Offset",params.batchnorm26.Offset,"Scale",params.batchnorm26.Scale,"TrainedMean",params.batchnorm26.TrainedMean,"TrainedVariance",params.batchnorm26.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu26")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        additionLayer(2,"Name","res11")
        convolution2dLayer([3 3],512,"Name","conv27","Padding",[1 0 1 0],"Stride",[2 2],"Bias",params.conv27.Bias,"Weights",params.conv27.Weights)
        batchNormalizationLayer("Name","batchnorm27","Offset",params.batchnorm27.Offset,"Scale",params.batchnorm27.Scale,"TrainedMean",params.batchnorm27.TrainedMean,"TrainedVariance",params.batchnorm27.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu27")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],256,"Name","conv28","Padding","same","Bias",params.conv28.Bias,"Weights",params.conv28.Weights)
        batchNormalizationLayer("Name","batchnorm28","Offset",params.batchnorm28.Offset,"Scale",params.batchnorm28.Scale,"TrainedMean",params.batchnorm28.TrainedMean,"TrainedVariance",params.batchnorm28.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu28")
        convolution2dLayer([3 3],512,"Name","conv29","Padding","same","Bias",params.conv29.Bias,"Weights",params.conv29.Weights)
        batchNormalizationLayer("Name","batchnorm29","Offset",params.batchnorm29.Offset,"Scale",params.batchnorm29.Scale,"TrainedMean",params.batchnorm29.TrainedMean,"TrainedVariance",params.batchnorm29.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu29")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res12");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],256,"Name","conv30","Padding","same","Bias",params.conv30.Bias,"Weights",params.conv30.Weights)
        batchNormalizationLayer("Name","batchnorm30","Offset",params.batchnorm30.Offset,"Scale",params.batchnorm30.Scale,"TrainedMean",params.batchnorm30.TrainedMean,"TrainedVariance",params.batchnorm30.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu30")
        convolution2dLayer([3 3],512,"Name","conv31","Padding","same","Bias",params.conv31.Bias,"Weights",params.conv31.Weights)
        batchNormalizationLayer("Name","batchnorm31","Offset",params.batchnorm31.Offset,"Scale",params.batchnorm31.Scale,"TrainedMean",params.batchnorm31.TrainedMean,"TrainedVariance",params.batchnorm31.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu31")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res13");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],256,"Name","conv32","Padding","same","Bias",params.conv32.Bias,"Weights",params.conv32.Weights)
        batchNormalizationLayer("Name","batchnorm32","Offset",params.batchnorm32.Offset,"Scale",params.batchnorm32.Scale,"TrainedMean",params.batchnorm32.TrainedMean,"TrainedVariance",params.batchnorm32.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu32")
        convolution2dLayer([3 3],512,"Name","conv33","Padding","same","Bias",params.conv33.Bias,"Weights",params.conv33.Weights)
        batchNormalizationLayer("Name","batchnorm33","Offset",params.batchnorm33.Offset,"Scale",params.batchnorm33.Scale,"TrainedMean",params.batchnorm33.TrainedMean,"TrainedVariance",params.batchnorm33.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu33")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res14");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],256,"Name","conv34","Padding","same","Bias",params.conv34.Bias,"Weights",params.conv34.Weights)
        batchNormalizationLayer("Name","batchnorm34","Offset",params.batchnorm34.Offset,"Scale",params.batchnorm34.Scale,"TrainedMean",params.batchnorm34.TrainedMean,"TrainedVariance",params.batchnorm34.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu34")
        convolution2dLayer([3 3],512,"Name","conv35","Padding","same","Bias",params.conv35.Bias,"Weights",params.conv35.Weights)
        batchNormalizationLayer("Name","batchnorm35","Offset",params.batchnorm35.Offset,"Scale",params.batchnorm35.Scale,"TrainedMean",params.batchnorm35.TrainedMean,"TrainedVariance",params.batchnorm35.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu35")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res15");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],256,"Name","conv36","Padding","same","Bias",params.conv36.Bias,"Weights",params.conv36.Weights)
        batchNormalizationLayer("Name","batchnorm36","Offset",params.batchnorm36.Offset,"Scale",params.batchnorm36.Scale,"TrainedMean",params.batchnorm36.TrainedMean,"TrainedVariance",params.batchnorm36.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu36")
        convolution2dLayer([3 3],512,"Name","conv37","Padding","same","Bias",params.conv37.Bias,"Weights",params.conv37.Weights)
        batchNormalizationLayer("Name","batchnorm37","Offset",params.batchnorm37.Offset,"Scale",params.batchnorm37.Scale,"TrainedMean",params.batchnorm37.TrainedMean,"TrainedVariance",params.batchnorm37.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu37")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res16");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],256,"Name","conv38","Padding","same","Bias",params.conv38.Bias,"Weights",params.conv38.Weights)
        batchNormalizationLayer("Name","batchnorm38","Offset",params.batchnorm38.Offset,"Scale",params.batchnorm38.Scale,"TrainedMean",params.batchnorm38.TrainedMean,"TrainedVariance",params.batchnorm38.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu38")
        convolution2dLayer([3 3],512,"Name","conv39","Padding","same","Bias",params.conv39.Bias,"Weights",params.conv39.Weights)
        batchNormalizationLayer("Name","batchnorm39","Offset",params.batchnorm39.Offset,"Scale",params.batchnorm39.Scale,"TrainedMean",params.batchnorm39.TrainedMean,"TrainedVariance",params.batchnorm39.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu39")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res17");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],256,"Name","conv40","Padding","same","Bias",params.conv40.Bias,"Weights",params.conv40.Weights)
        batchNormalizationLayer("Name","batchnorm40","Offset",params.batchnorm40.Offset,"Scale",params.batchnorm40.Scale,"TrainedMean",params.batchnorm40.TrainedMean,"TrainedVariance",params.batchnorm40.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu40")
        convolution2dLayer([3 3],512,"Name","conv41","Padding","same","Bias",params.conv41.Bias,"Weights",params.conv41.Weights)
        batchNormalizationLayer("Name","batchnorm41","Offset",params.batchnorm41.Offset,"Scale",params.batchnorm41.Scale,"TrainedMean",params.batchnorm41.TrainedMean,"TrainedVariance",params.batchnorm41.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu41")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res18");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],256,"Name","conv42","Padding","same","Bias",params.conv42.Bias,"Weights",params.conv42.Weights)
        batchNormalizationLayer("Name","batchnorm42","Offset",params.batchnorm42.Offset,"Scale",params.batchnorm42.Scale,"TrainedMean",params.batchnorm42.TrainedMean,"TrainedVariance",params.batchnorm42.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu42")
        convolution2dLayer([3 3],512,"Name","conv43","Padding","same","Bias",params.conv43.Bias,"Weights",params.conv43.Weights)
        batchNormalizationLayer("Name","batchnorm43","Offset",params.batchnorm43.Offset,"Scale",params.batchnorm43.Scale,"TrainedMean",params.batchnorm43.TrainedMean,"TrainedVariance",params.batchnorm43.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu43")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        additionLayer(2,"Name","res19")
        convolution2dLayer([3 3],1024,"Name","conv44","Padding",[1 0 1 0],"Stride",[2 2],"Bias",params.conv44.Bias,"Weights",params.conv44.Weights)
        batchNormalizationLayer("Name","batchnorm44","Offset",params.batchnorm44.Offset,"Scale",params.batchnorm44.Scale,"TrainedMean",params.batchnorm44.TrainedMean,"TrainedVariance",params.batchnorm44.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu44")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],512,"Name","conv45","Padding","same","Bias",params.conv45.Bias,"Weights",params.conv45.Weights)
        batchNormalizationLayer("Name","batchnorm45","Offset",params.batchnorm45.Offset,"Scale",params.batchnorm45.Scale,"TrainedMean",params.batchnorm45.TrainedMean,"TrainedVariance",params.batchnorm45.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu45")
        convolution2dLayer([3 3],1024,"Name","conv46","Padding","same","Bias",params.conv46.Bias,"Weights",params.conv46.Weights)
        batchNormalizationLayer("Name","batchnorm46","Offset",params.batchnorm46.Offset,"Scale",params.batchnorm46.Scale,"TrainedMean",params.batchnorm46.TrainedMean,"TrainedVariance",params.batchnorm46.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu46")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res20");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],512,"Name","conv47","Padding","same","Bias",params.conv47.Bias,"Weights",params.conv47.Weights)
        batchNormalizationLayer("Name","batchnorm47","Offset",params.batchnorm47.Offset,"Scale",params.batchnorm47.Scale,"TrainedMean",params.batchnorm47.TrainedMean,"TrainedVariance",params.batchnorm47.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu47")
        convolution2dLayer([3 3],1024,"Name","conv48","Padding","same","Bias",params.conv48.Bias,"Weights",params.conv48.Weights)
        batchNormalizationLayer("Name","batchnorm48","Offset",params.batchnorm48.Offset,"Scale",params.batchnorm48.Scale,"TrainedMean",params.batchnorm48.TrainedMean,"TrainedVariance",params.batchnorm48.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu48")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res21");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],512,"Name","conv49","Padding","same","Bias",params.conv49.Bias,"Weights",params.conv49.Weights)
        batchNormalizationLayer("Name","batchnorm49","Offset",params.batchnorm49.Offset,"Scale",params.batchnorm49.Scale,"TrainedMean",params.batchnorm49.TrainedMean,"TrainedVariance",params.batchnorm49.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu49")
        convolution2dLayer([3 3],1024,"Name","conv50","Padding","same","Bias",params.conv50.Bias,"Weights",params.conv50.Weights)
        batchNormalizationLayer("Name","batchnorm50","Offset",params.batchnorm50.Offset,"Scale",params.batchnorm50.Scale,"TrainedMean",params.batchnorm50.TrainedMean,"TrainedVariance",params.batchnorm50.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu50")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = additionLayer(2,"Name","res22");
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        convolution2dLayer([1 1],512,"Name","conv51","Padding","same","Bias",params.conv51.Bias,"Weights",params.conv51.Weights)
        batchNormalizationLayer("Name","batchnorm51","Offset",params.batchnorm51.Offset,"Scale",params.batchnorm51.Scale,"TrainedMean",params.batchnorm51.TrainedMean,"TrainedVariance",params.batchnorm51.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu51")
        convolution2dLayer([3 3],1024,"Name","conv52","Padding","same","Bias",params.conv52.Bias,"Weights",params.conv52.Weights)
        batchNormalizationLayer("Name","batchnorm52","Offset",params.batchnorm52.Offset,"Scale",params.batchnorm52.Scale,"TrainedMean",params.batchnorm52.TrainedMean,"TrainedVariance",params.batchnorm52.TrainedVariance)
        leakyReluLayer(0.1,"Name","leakyrelu52")];
    lgraph = addLayers(lgraph,tempLayers);
    
    tempLayers = [
        additionLayer(2,"Name","res23")
        globalAveragePooling2dLayer("Name","avg1")
        convolution2dLayer([1 1],1000,"Name","conv53","Padding","same","Bias",params.conv53.Bias,"Weights",params.conv53.Weights)
        fullyConnectedLayer(num_outputs,"Name","fc")
        regressionLayer("Name","regressionoutput")];
    lgraph = addLayers(lgraph,tempLayers);
    
    % clean up helper variable
    clear tempLayers;
    
    % Connect Layer Branches
    % Connect all the branches of the network to create the network graph.
    lgraph = connectLayers(lgraph,"leakyrelu2","conv3");
    lgraph = connectLayers(lgraph,"leakyrelu2","res1/in2");
    lgraph = connectLayers(lgraph,"leakyrelu4","res1/in1");
    lgraph = connectLayers(lgraph,"leakyrelu5","conv6");
    lgraph = connectLayers(lgraph,"leakyrelu5","res2/in2");
    lgraph = connectLayers(lgraph,"leakyrelu7","res2/in1");
    lgraph = connectLayers(lgraph,"res2","conv8");
    lgraph = connectLayers(lgraph,"res2","res3/in2");
    lgraph = connectLayers(lgraph,"leakyrelu9","res3/in1");
    lgraph = connectLayers(lgraph,"leakyrelu10","conv11");
    lgraph = connectLayers(lgraph,"leakyrelu10","res4/in2");
    lgraph = connectLayers(lgraph,"leakyrelu12","res4/in1");
    lgraph = connectLayers(lgraph,"res4","conv13");
    lgraph = connectLayers(lgraph,"res4","res5/in2");
    lgraph = connectLayers(lgraph,"leakyrelu14","res5/in1");
    lgraph = connectLayers(lgraph,"res5","conv15");
    lgraph = connectLayers(lgraph,"res5","res6/in2");
    lgraph = connectLayers(lgraph,"leakyrelu16","res6/in1");
    lgraph = connectLayers(lgraph,"res6","conv17");
    lgraph = connectLayers(lgraph,"res6","res7/in2");
    lgraph = connectLayers(lgraph,"leakyrelu18","res7/in1");
    lgraph = connectLayers(lgraph,"res7","conv19");
    lgraph = connectLayers(lgraph,"res7","res8/in2");
    lgraph = connectLayers(lgraph,"leakyrelu20","res8/in1");
    lgraph = connectLayers(lgraph,"res8","conv21");
    lgraph = connectLayers(lgraph,"res8","res9/in2");
    lgraph = connectLayers(lgraph,"leakyrelu22","res9/in1");
    lgraph = connectLayers(lgraph,"res9","conv23");
    lgraph = connectLayers(lgraph,"res9","res10/in2");
    lgraph = connectLayers(lgraph,"leakyrelu24","res10/in1");
    lgraph = connectLayers(lgraph,"res10","conv25");
    lgraph = connectLayers(lgraph,"res10","res11/in2");
    lgraph = connectLayers(lgraph,"leakyrelu26","res11/in1");
    lgraph = connectLayers(lgraph,"leakyrelu27","conv28");
    lgraph = connectLayers(lgraph,"leakyrelu27","res12/in2");
    lgraph = connectLayers(lgraph,"leakyrelu29","res12/in1");
    lgraph = connectLayers(lgraph,"res12","conv30");
    lgraph = connectLayers(lgraph,"res12","res13/in2");
    lgraph = connectLayers(lgraph,"leakyrelu31","res13/in1");
    lgraph = connectLayers(lgraph,"res13","conv32");
    lgraph = connectLayers(lgraph,"res13","res14/in2");
    lgraph = connectLayers(lgraph,"leakyrelu33","res14/in1");
    lgraph = connectLayers(lgraph,"res14","conv34");
    lgraph = connectLayers(lgraph,"res14","res15/in2");
    lgraph = connectLayers(lgraph,"leakyrelu35","res15/in1");
    lgraph = connectLayers(lgraph,"res15","conv36");
    lgraph = connectLayers(lgraph,"res15","res16/in2");
    lgraph = connectLayers(lgraph,"leakyrelu37","res16/in1");
    lgraph = connectLayers(lgraph,"res16","conv38");
    lgraph = connectLayers(lgraph,"res16","res17/in2");
    lgraph = connectLayers(lgraph,"leakyrelu39","res17/in1");
    lgraph = connectLayers(lgraph,"res17","conv40");
    lgraph = connectLayers(lgraph,"res17","res18/in2");
    lgraph = connectLayers(lgraph,"leakyrelu41","res18/in1");
    lgraph = connectLayers(lgraph,"res18","conv42");
    lgraph = connectLayers(lgraph,"res18","res19/in2");
    lgraph = connectLayers(lgraph,"leakyrelu43","res19/in1");
    lgraph = connectLayers(lgraph,"leakyrelu44","conv45");
    lgraph = connectLayers(lgraph,"leakyrelu44","res20/in2");
    lgraph = connectLayers(lgraph,"leakyrelu46","res20/in1");
    lgraph = connectLayers(lgraph,"res20","conv47");
    lgraph = connectLayers(lgraph,"res20","res21/in2");
    lgraph = connectLayers(lgraph,"leakyrelu48","res21/in1");
    lgraph = connectLayers(lgraph,"res21","conv49");
    lgraph = connectLayers(lgraph,"res21","res22/in2");
    lgraph = connectLayers(lgraph,"leakyrelu50","res22/in1");
    lgraph = connectLayers(lgraph,"res22","conv51");
    lgraph = connectLayers(lgraph,"res22","res23/in2");
    lgraph = connectLayers(lgraph,"leakyrelu52","res23/in1");
    
%     Plot Layers
%     plot(lgraph);

end